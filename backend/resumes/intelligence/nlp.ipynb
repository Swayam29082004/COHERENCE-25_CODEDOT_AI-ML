{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "import spacy\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.3/12.8 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.5/12.8 MB 1.2 MB/s eta 0:00:11\n",
      "     -- ------------------------------------- 0.8/12.8 MB 1.2 MB/s eta 0:00:11\n",
      "     --- ------------------------------------ 1.0/12.8 MB 1.2 MB/s eta 0:00:11\n",
      "     ---- ----------------------------------- 1.3/12.8 MB 1.2 MB/s eta 0:00:10\n",
      "     ---- ----------------------------------- 1.6/12.8 MB 1.1 MB/s eta 0:00:11\n",
      "     ----- ---------------------------------- 1.8/12.8 MB 1.1 MB/s eta 0:00:10\n",
      "     ------ --------------------------------- 2.1/12.8 MB 1.2 MB/s eta 0:00:10\n",
      "     --------- ------------------------------ 2.9/12.8 MB 1.4 MB/s eta 0:00:07\n",
      "     ----------- ---------------------------- 3.7/12.8 MB 1.7 MB/s eta 0:00:06\n",
      "     ------------- -------------------------- 4.2/12.8 MB 1.8 MB/s eta 0:00:05\n",
      "     --------------- ------------------------ 5.0/12.8 MB 2.0 MB/s eta 0:00:04\n",
      "     ------------------ --------------------- 5.8/12.8 MB 2.1 MB/s eta 0:00:04\n",
      "     -------------------- ------------------- 6.6/12.8 MB 2.2 MB/s eta 0:00:03\n",
      "     ---------------------- ----------------- 7.3/12.8 MB 2.3 MB/s eta 0:00:03\n",
      "     ------------------------- -------------- 8.1/12.8 MB 2.4 MB/s eta 0:00:02\n",
      "     --------------------------- ------------ 8.9/12.8 MB 2.5 MB/s eta 0:00:02\n",
      "     ---------------------------- ----------- 9.2/12.8 MB 2.5 MB/s eta 0:00:02\n",
      "     ------------------------------- -------- 10.0/12.8 MB 2.5 MB/s eta 0:00:02\n",
      "     -------------------------------- ------- 10.5/12.8 MB 2.6 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.5/12.8 MB 2.6 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.3/12.8 MB 2.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 2.7 MB/s eta 0:00:00\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\envs\\commai\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Load spaCy's pre-trained model\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        # Try direct text extraction\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text += page_text\n",
    "\n",
    "        if text.strip():\n",
    "            return text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Direct text extraction failed: {e}\")\n",
    "\n",
    "    # Fallback to OCR for image-based PDFs\n",
    "    print(\"Falling back to OCR for image-based PDF.\")\n",
    "    try:\n",
    "        images = convert_from_path(pdf_path)\n",
    "        for image in images:\n",
    "            page_text = pytesseract.image_to_string(image)\n",
    "            text += page_text + \"\\n\"\n",
    "    except Exception as e:\n",
    "        print(f\"OCR failed: {e}\")\n",
    "\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Text from PDF:\n",
      "BARI ANKIT VINOD\n",
      "github.com/OnlyCR7 | linkedin.com/in/mycr7/ | vbari8527@gmail.com | +91-7875618947\n",
      "Portfolio Website : https://onlycr7.github.io/DataSci_Labs_Portfolio/\n",
      "Skills\n",
      "Languages: C/C++, Java, JavaScript, SQL, Python\n",
      "Technologies & Tools: DBMS, Tableau, PowerBI, Docker, ML & DL, NLP, Visualization, Computer Vision, TensorFlow,\n",
      "PyTorch, LLMs\n",
      "Education\n",
      "Theem College of Engg. 2021 - 2022 - 2023\n",
      "Diploma in Computer Science and Engineering Percentage: 85%\n",
      "Relevant Coursework: Object Oriented Programming, Databases, Data Structures and Algorithms, Operating Systems, Computer\n",
      "Networks, Computer Graphics\n",
      "Vidyavardhini’s College of Engg. And Tech. 2023 - Now\n",
      "B.E. in Artificial Intelligence and Data Science\n",
      "Relevant Coursework: Object Oriented Programming, Databases, Discrete Maths, Applied Maths, Data Structures and Analysis\n",
      "of Algorithms, Operating Systems, Computer Networks, Machine Learning, Data Mining, Advance Data Structures and\n",
      "Algorithms, Information Retrieval, Image Processing, NLP\n",
      "Project Work\n",
      "• Land Area Calculation using ML (2022): Developed a desktop software for calculating the land area in map. Implemented\n",
      "efficient to use for the user. Used openstreet map API to get the online map. Python, OpenStreetMap.\n",
      "• Smart Parking System using CV (2023): Applied computer vision concepts to find the correct parking space for the car.\n",
      "Implemented front-end to book a slot for the car to park and integrate system with back-end. Used cv2 library for recognition.\n",
      "Python, CV2.\n",
      "• Smart Driving System using NLP (2024): Used the computer vision to detect the path holes and using NLP concepts\n",
      "implemented the correct path to drive the car. Python, CV2, NLP.\n",
      "Certificates\n",
      "• Internship in Data Science: Successfully completed the training and internship in domain of Data Science by\n",
      "Acmegrade.\n"
     ]
    }
   ],
   "source": [
    "pdf_path = r\"C:\\Users\\Admin\\OneDrive\\Desktop\\codedot\\backend\\media\\resume\\bariankit_btech.pdf\"\n",
    "resume_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "print(\"\\nExtracted Text from PDF:\")\n",
    "print(resume_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract Name using SpaCy NER\n",
    "def extract_name(text):\n",
    "    # Process the text with SpaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Loop through named entities detected by SpaCy\n",
    "    for ent in doc.ents:\n",
    "        # If the entity is a person, return the name\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            return ent.text\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract emails using spaCy and regex\n",
    "def extract_email(text):\n",
    "    # Use spaCy to process the text\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Find all potential email matches using regex\n",
    "    emails = re.findall(r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\", text)\n",
    "    \n",
    "    # If emails are found, return the list, otherwise return None\n",
    "    return emails if emails else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean the text using spaCy\n",
    "def clean_text(text):\n",
    "    # Process text with spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Remove non-alphanumeric characters (excluding spaces and dashes in phone numbers)\n",
    "    cleaned_text = ' '.join([token.text for token in doc if token.is_alpha or token.is_space or token.text in ['-', '+']])\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "# Function to extract phone number using Regex\n",
    "def extract_phone(text):\n",
    "    # Clean the text first\n",
    "    cleaned_text = clean_text(text)\n",
    "    \n",
    "    # Use regex to find phone number patterns\n",
    "    phone = re.search(r\"\\+?\\d[\\d\\s\\-\\(\\)]{9,}\\d\", cleaned_text)\n",
    "    \n",
    "    return phone.group(0) if phone else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract skills using NLP techniques\n",
    "def extract_skills(text):\n",
    "    # Apply spaCy NLP pipeline\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # List of possible skill-related categories or contexts\n",
    "    skill_keywords = [\"programming\", \"language\", \"tool\", \"technology\", \"framework\", \"library\", \"platform\"]\n",
    "    \n",
    "    # Extract noun phrases that might represent skills\n",
    "    skills = set()  # Using a set to avoid duplicates\n",
    "    \n",
    "    for np in doc.noun_chunks:\n",
    "        # We look for noun phrases that might represent skills (e.g., programming languages, tools, etc.)\n",
    "        if any(keyword in np.text.lower() for keyword in skill_keywords):\n",
    "            skills.add(np.text.strip())\n",
    "    \n",
    "    # Filter the list to make it more meaningful (optional)\n",
    "    # You can add logic to further filter based on predefined skill patterns\n",
    "    \n",
    "    return list(skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_education(text):\n",
    "    # Use spaCy to process the text\n",
    "    doc = nlp(text)\n",
    "    education = []\n",
    "\n",
    "    # Define regular expressions for patterns like degrees, years, etc.\n",
    "    degree_keywords = r\"\\b(Bachelor|Master|PhD|Diploma|Degree|Engineering|Science|Arts)\\b\"\n",
    "    institution_keywords = r\"\\b(College|University|Institute|Academy)\\b\"\n",
    "    \n",
    "    # Iterate over sentences\n",
    "    for sent in doc.sents:\n",
    "        # Extract sentences with keywords related to education\n",
    "        if re.search(degree_keywords, sent.text, re.IGNORECASE) or re.search(institution_keywords, sent.text, re.IGNORECASE):\n",
    "            education.append(sent.text.strip())\n",
    "\n",
    "    # Further process education entries to refine results (like adding the institution, degree, and years)\n",
    "    refined_education = []\n",
    "    for entry in education:\n",
    "        # Extract degree, institution, and year/duration using regex\n",
    "        degree_match = re.search(degree_keywords, entry)\n",
    "        institution_match = re.search(institution_keywords, entry)\n",
    "        year_match = re.search(r\"\\d{4}(-\\d{4})?\", entry)\n",
    "\n",
    "        # Build a structured education entry\n",
    "        education_entry = {}\n",
    "\n",
    "        if degree_match:\n",
    "            education_entry[\"Degree\"] = degree_match.group(0)\n",
    "        if institution_match:\n",
    "            education_entry[\"Institution\"] = institution_match.group(0)\n",
    "        if year_match:\n",
    "            education_entry[\"Year\"] = year_match.group(0)\n",
    "\n",
    "        if education_entry:\n",
    "            refined_education.append(education_entry)\n",
    "\n",
    "    return refined_education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean and extract Experience using NLP\n",
    "def extract_experience(text):\n",
    "    # Step 1: Preprocess the text\n",
    "    text = text.replace(\"\\n\", \" \")  # Replace newlines with space\n",
    "    doc = nlp(text)  # Parse the text using spaCy NLP model\n",
    "    \n",
    "    # Step 2: Identify sentences with experience-related terms\n",
    "    experience_sentences = []\n",
    "    \n",
    "    # We'll look for common phrases related to work experience, projects, and roles\n",
    "    experience_keywords = ['project', 'work', 'experience', 'role', 'responsibilities', 'intern', 'development', 'managed']\n",
    "    \n",
    "    for sent in doc.sents:\n",
    "        if any(keyword in sent.text.lower() for keyword in experience_keywords):\n",
    "            experience_sentences.append(sent.text.strip())\n",
    "    \n",
    "    # Step 3: Further clean up by extracting relevant entities like organizations, roles, and dates\n",
    "    experience_data = []\n",
    "    for sent in experience_sentences:\n",
    "        entities = []\n",
    "        for ent in nlp(sent).ents:\n",
    "            if ent.label_ in [\"ORG\", \"DATE\", \"GPE\", \"PERSON\", \"WORK_OF_ART\"]:  # Look for orgs, dates, roles\n",
    "                entities.append(ent.text)\n",
    "        \n",
    "        # Only append sentences with some useful entities\n",
    "        if entities:\n",
    "            experience_data.append({\"sentence\": sent, \"entities\": entities})\n",
    "    \n",
    "    return experience_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call functions and print the extracted details\n",
    "name = extract_name(resume_text)\n",
    "email = extract_email(resume_text)\n",
    "phone = extract_phone(resume_text)\n",
    "skills = extract_skills(resume_text)\n",
    "experience = extract_experience(resume_text)\n",
    "education = extract_education(resume_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Quizz Website\n",
      "Email: ['vbari8527@gmail.com']\n",
      "Phone: None\n",
      "Skills: ['programming languages', 'programming section', 'programming part']\n",
      "Experience: [{'sentence': 'ANKIT BARI PROFILE EDUCATION BARI ANKIT VINOD Diploma in Computer Engineering-Theem College 2021 - 2023 CS Student I learn more about networking, programming languages and databases.', 'entities': ['ANKIT', 'Computer Engineering-Theem College', '2021 - 2023']}, {'sentence': 'Palghar Gungwada PROJECT EXPERIENCE Pin - 401601 Land Area Calculation in Python (I am performing programming part of our project.)', 'entities': ['Palghar Gungwada PROJECT']}, {'sentence': 'We are getting API key of Google Map and using some Machine Learning concepts we are performed this project.', 'entities': ['API', 'Machine Learning']}, {'sentence': 'Quizz Website in Html,CSS and JS (I am doing programming section of our project.)', 'entities': ['Quizz Website', 'Html', 'CSS']}, {'sentence': 'Python Programming Learn New Skills Attendance Management in MySQL Problem Solving (I am worked as a back end for this project.)', 'entities': ['Python Programming Learn New Skills Attendance Management', 'Problem Solving']}]\n",
      "Education: [{'Degree': 'Diploma', 'Institution': 'College', 'Year': '2021'}, {'Degree': 'Degree'}, {'Institution': 'College', 'Year': '2023'}]\n"
     ]
    }
   ],
   "source": [
    "# Print the results\n",
    "print(\"Name:\", name)\n",
    "print(\"Email:\", email)\n",
    "print(\"Phone:\", phone)\n",
    "print(\"Skills:\", skills)\n",
    "print(\"Experience:\", experience)\n",
    "print(\"Education:\", education)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "commai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
